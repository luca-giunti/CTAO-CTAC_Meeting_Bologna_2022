{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **A full 1D analysis using the low-level Gammapy API**\n",
    "\n",
    "**Objective: Performing a full spectral anaysis of the point source [PKS 2155-304](http://tevcat.uchicago.edu/?mode=1&showsrc=90)**\n",
    "\n",
    "Here we demonstrate the data reduction and spectral fitting for a point like source, using the [reflected regions background estimation](https://docs.gammapy.org/0.20/makers/reflected.html?highlight=finder) method.\n",
    "\n",
    "In practice, we have to:\n",
    "- Create a `~gammapy.data.DataStore` poiting to the relevant data \n",
    "- Apply an observation selection to produce a list of observations, a `~gammapy.data.Observations` object.\n",
    "- Define the [reconstructed energy](https://docs.gammapy.org/0.20/userguide/references.html#term-Reco-Energy) axis and [true energy](https://docs.gammapy.org/0.20/userguide/references.html#term-True-Energy) axis using the `~gammapy.maps.MapAxis` object\n",
    "- Create the necessary makers : \n",
    "    - the spectrum dataset maker : `~gammapy.makers.SpectrumDatasetMaker`\n",
    "    - the reflected regions finder: either `~gammapy.makers.ReflectedRegionsFinder` or `~gammapy.makers.WobbleRegionsFinder`\n",
    "    - the background maker, here a `~gammapy.makers.ReflectedRegionsBackgroundMaker`\n",
    "    - and usually the safe range maker : `~gammapy.makers.SafeMaskMaker`\n",
    "- Perform the data reduction loop. And for every observation:\n",
    "    - Apply the makers sequentially to produce the current `~gammapy.datasets.SpectrumDatasetOnOff`\n",
    "    - Stack it on the target one.\n",
    "- Define the `~gammapy.modeling.models.SkyModel` to fit the data. Being this a spectral analysis, the `SkyModel` is completely defined by a  `~gammapy.modeling.models.SpectralModel` (no spatial information is required)\n",
    "- Create a `~gammapy.modeling.Fit` object and run it to fit the model parameters\n",
    "- Apply a `~gammapy.estimators.FluxPointsEstimator` to compute flux points for the spectral part of the fit.\n",
    "\n",
    "## Setup\n",
    "First, we setup the analysis by performing required imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "from gammapy.irf import load_cta_irfs\n",
    "from gammapy.maps import WcsGeom, MapAxis, RegionGeom\n",
    "from gammapy.modeling.models import (\n",
    "    PowerLawSpectralModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    PointSpatialModel,\n",
    "    SkyModel,\n",
    "    Models,\n",
    "    FoVBackgroundModel,\n",
    "    LogParabolaSpectralModel,\n",
    "    EBLAbsorptionNormSpectralModel\n",
    ")\n",
    "from gammapy.makers import SafeMaskMaker, SpectrumDatasetMaker, ReflectedRegionsFinder, ReflectedRegionsBackgroundMaker\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.datasets import MapDataset, FluxPointsDataset, Datasets, SpectrumDataset\n",
    "from scipy.stats import chi2\n",
    "from gammapy.catalog import SourceCatalog4FGL\n",
    "\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from regions import CircleSkyRegion\n",
    "from gammapy.estimators import FluxPointsEstimator\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.visualization import plot_spectrum_datasets_off_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the datastore and selecting observations\n",
    "\n",
    "We first use the `~gammapy.data.DataStore` object to access the observations we want to analyse. Here the H.E.S.S. DL3 DR1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = DataStore.from_dir(\"$GAMMAPY_DATA/hess-dl3-dr1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = SkyCoord(329.71693844, -30.22558846, unit=u.deg, frame=\"icrs\")\n",
    "pos.icrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define an observation filter to select only the relevant observations. \n",
    "Here we use a cone search which we define with a python dict.\n",
    "\n",
    "We then filter the `ObservationTable` with `~gammapy.data.ObservationTable.select_sky_circle()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_table_filtered = data_store.obs_table.select_sky_circle(center=pos, radius=2 * u.deg)\n",
    "obs_ids = obs_table_filtered[\"OBS_ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to apply more complex filtering options, you can use the `~gammapy.data.ObservationTable.select_observations()` method instead. This provides the freedom of selecting observations based on a sky circle, time period or parameter (e.g. Zenith angle) range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve the relevant observations by passing their `obs_id` to the`~gammapy.data.DataStore.get_observations()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = data_store.get_observations(obs_ids)\n",
    "print(f\"Number of selected observations : {len(observations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict the analysis to the [July 2006 flaring event](https://ui.adsabs.harvard.edu/abs/2009A%26A...502..749A/abstract) using `gammapy.data.Observations.select_time()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_interval = Time(\n",
    "    [\"2006-07-29T20:30\", \"2006-07-30T20:30\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = observations.select_time(time_interval)\n",
    "print(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = observations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.events.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing reduced datasets geometry\n",
    "\n",
    "Now we define the [reconstructed](https://docs.gammapy.org/0.20/userguide/references.html#term-Reco-Energy) and [true](https://docs.gammapy.org/0.20/userguide/references.html#term-True-Energy) energy axes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    0.1, 40, nbin=10, per_decade=True, unit=\"TeV\", name=\"energy\"\n",
    ")\n",
    "energy_axis_true = MapAxis.from_energy_bounds(\n",
    "    0.05, 100, nbin=20, per_decade=True, unit=\"TeV\", name=\"energy_true\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a ON region to extract the spectrum, and create the analysis geometry using the `RegionGeom` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_region_radius = Angle(\"0.11 deg\")\n",
    "on_region = CircleSkyRegion(center=pos, radius=on_region_radius)\n",
    "\n",
    "geom = RegionGeom.create(region=on_region, axes=[energy_axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the target dataset with this geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_empty = SpectrumDataset.create(\n",
    "    geom=geom, energy_axis_true=energy_axis_true\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create exclusion mask\n",
    "\n",
    "To perform the spectral analysis we must mask all the gamma ray emission in the analysis region, which would otherwise bias the background estimation. Here we are analyzing an extra-Galactic source, which is isolated and would not require a priori an exclusion mask. However, for illustration purpose, we choose a mask of 0.5 deg to the North of the blazar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_region = CircleSkyRegion(\n",
    "    center=SkyCoord(329.71, -29.5, unit=\"deg\", frame=\"icrs\"),\n",
    "    radius=0.5 * u.deg,\n",
    ")\n",
    "\n",
    "skydir = pos.icrs\n",
    "geom = WcsGeom.create(width=5*u.deg, binsz=0.05, skydir=skydir, proj=\"TAN\", frame=\"icrs\"\n",
    ")\n",
    "\n",
    "exclusion_mask = ~geom.region_mask([exclusion_region])\n",
    "exclusion_mask.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reduction\n",
    "\n",
    "### Create the maker classes to be used\n",
    "We first initialize the `Maker` objects that will take care of the data reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `~gammapy.makers.SpectrumDatasetMaker`creates a `SpectrumDataset` for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_maker = SpectrumDatasetMaker(\n",
    "    containment_correction=True, selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `~gammapy.makers.ReflectedRegionsBackgroundMaker` appends a background estimate (based on the [reflected regions](https://docs.gammapy.org/0.20/makers/reflected.html?highlight=finder) method) to an input `SpectrumDataset`, converting it into a `SpectrumDatasetOnOff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_maker = ReflectedRegionsBackgroundMaker(exclusion_mask=exclusion_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `ReflectedRegionsBackgroundMaker` defines the OFF regions by rotating the ON region around the pointing position. If you need to apply more complex criteria to your OFF regions selection (e.g. an energy dependent rad-max cut / configure the number of OFF regions, etc) you can additionally pass to the `ReflectedRegionsBackgroundMaker` an instance of `~gammapy.makers.WobbleRegionsFinder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define a `~gammapy.makers.SafeMaskMaker` instance, which is responsible of selecting the safe data range (in energy and space) in which the data can be used. In this example we only use the method `aeff-default`, which reads the safe energy threshold specified in the DL3 FITS files. For other available method see https://docs.gammapy.org/0.20/api/gammapy.makers.SafeMaskMaker.html?highlight=safemaskmaker#gammapy.makers.SafeMaskMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_mask_masker = SafeMaskMaker(methods=[\"aeff-default\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the data reduction loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment the datasets are not stacked, but appended into a `Datasets` object (which basically contains a list of datasets). That's because we want to produce diagnostic plots such as the cumulative source significance as a function of the observation livetime. The stacking will be performed later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "datasets = Datasets()\n",
    "for observation in observations:\n",
    "    # First a spectrum dataset with the same geometry as the reference empty one is filled with the data and IRFs\n",
    "    dataset = dataset_maker.run(\n",
    "        dataset_empty.copy(name=observation.obs_id), observation)\n",
    "    # Reflected regions background estimation\n",
    "    dataset_on_off = bkg_maker.run(dataset, observation)\n",
    "    # The data quality cut is applied\n",
    "    dataset_on_off = safe_mask_masker.run(dataset_on_off, observation)\n",
    "    # The resulting dataset is appended to the list\n",
    "    datasets.append(dataset_on_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = exclusion_mask.plot()\n",
    "plot_spectrum_datasets_off_regions(ax=ax, datasets=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = datasets.info_table(cumulative=True)\n",
    "info_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.subplot(221)\n",
    "ax1.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"), info_table[\"excess\"], marker=\"o\", ls=\"none\"\n",
    ")\n",
    "ax1.set_xlabel(\"Livetime [h]\")\n",
    "ax1.set_ylabel(\"Excess\");\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(222)\n",
    "ax2.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"),\n",
    "    info_table[\"sqrt_ts\"],\n",
    "    marker=\"o\",\n",
    "    ls=\"none\",\n",
    ")\n",
    "ax2.set_xlabel(\"Livetime [h]\")\n",
    "ax2.set_ylabel(\"Sqrt(TS)\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset to disc using `~gammapy.datasets.Datasets.write()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"pks-joint-dataset.yaml\"\n",
    "datasets.write(filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting we stack the `Datasets` into a single `SpectrumDatasetOnOff`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stacked\n",
    "stacked = datasets.stack_reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fitting\n",
    "\n",
    "In this section we fit a spectral model to the data. We can try to answer the following questions:\n",
    "- What is the significance of the detected source?\n",
    "- What is the best spectral shape to describe the spectrum of the source? \n",
    "\n",
    "In particular, we can use the [likelihood ratio test](https://docs.gammapy.org/0.20/userguide/stats.html#estimating-ts) to compare three different hypotheses:\n",
    "- H0: Background only (no source)\n",
    "- H1: Background + source described by a power law model\n",
    "- H2: Background + source described by a power law model with exponential cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the quantity $-2\\ln\\mathcal(L)$ for the background-only model (null hypothesis) can be simply computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wstat_0 = stacked.stat_sum()\n",
    "print(Wstat_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the background has been estimated using the reflected regions method, here $-2\\ln\\mathcal(L)$ corresponds to the so-called [Wstat](https://docs.gammapy.org/0.20/userguide/references.html#term-WStat) fit statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the model residuals for the H0 hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.plot_residuals_spectral()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the residuals show a clear positive feature indicating that a source is missing in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **H1**\n",
    "We now add a source defined by a power law spectrum to the model.\n",
    "\n",
    "Here we also consider [EBL absorption](https://docs.gammapy.org/0.20/modeling/gallery/spectral/plot_absorbed.html?highlight=ebl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_model_pl = PowerLawSpectralModel()\n",
    "redshift = 0.116\n",
    "ebl = EBLAbsorptionNormSpectralModel.read_builtin(\"dominguez\", redshift=redshift)\n",
    "spectral_model_1 = spectral_model_pl * ebl\n",
    "\n",
    "pks_model_1 = SkyModel(spectral_model=spectral_model_1,\n",
    "                    name=\"pks_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pks_model_1.parameters.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.models = [pks_model_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit1 = Fit(optimize_opts={\"print_level\": 1})\n",
    "result1 = fit1.run(datasets=[stacked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.models.to_parameters_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wstat_1 = result1.total_stat\n",
    "print(\"delta TS of detection: \", (Wstat_0-Wstat_1), \"p-value: \", chi2.sf((Wstat_0-Wstat_1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_spectrum, ax_residuals = stacked.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute flux points for the H0 model assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_edges = np.logspace(-1, 1.6, 12)*u.TeV\n",
    "fpe = FluxPointsEstimator(energy_edges=energy_edges, source=pks_model_1.name, selection_optional=[\"ul\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we activate the progress bar functionality, which can be helpful for time-consuming tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammapy.utils import pbar\n",
    "pbar.SHOW_PROGRESS_BAR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flux_points = fpe.run(datasets=[stacked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = spectral_model_pl.plot(energy_bounds=[0.1,20]*u.TeV, sed_type=\"e2dnde\", label=\"intrinsic\", color=\"blue\")\n",
    "spectral_model_1.plot(ax=ax, energy_bounds=[0.1,20]*u.TeV, sed_type=\"e2dnde\", label=\"absorbed\", color=\"red\")\n",
    "spectral_model_1.plot_error(ax=ax, energy_bounds=[0.1,20]*u.TeV, sed_type=\"e2dnde\", facecolor=\"red\")\n",
    "flux_points.plot(ax=ax, sed_type=\"e2dnde\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_model_1.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **H2**\n",
    "\n",
    "We now estimate the significance for the presence of an exponential cutoff in the source spectrum, again taking into account the EBL absorption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_model_ecpl = ExpCutoffPowerLawSpectralModel()\n",
    "redshift = 0.116\n",
    "spectral_model_2 = spectral_model_ecpl * ebl\n",
    "\n",
    "pks_model_2 = SkyModel(spectral_model=spectral_model_2,\n",
    "                    name=\"pks_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pks_model_2.parameters.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.models = [pks_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit2 = Fit(optimize_opts={\"print_level\": 1})\n",
    "result2 = fit2.run(datasets=[stacked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.models.to_parameters_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wstat_2 = result2.total_stat\n",
    "print(\"delta TS of detection: \", (Wstat_1-Wstat_2), \"p-value: \", chi2.sf((Wstat_1-Wstat_2), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully detected a cutoff in the observed spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_spectrum, ax_residuals = stacked.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the fit has coverged correctly, it is always a good idea to inspect the likelihood profile for the free model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stat = result2.total_stat\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "\n",
    "for ax, par in zip(axes, pks_model_2.parameters.free_parameters):\n",
    "    par.scan_n_values = 17\n",
    "\n",
    "    profile = fit2.stat_profile(datasets=[stacked], parameter=par)\n",
    "    ax.plot(profile[f\"{par.name}_scan\"], profile[\"stat_scan\"] - total_stat)\n",
    "    ax.set_xlabel(f\"{par.unit}\")\n",
    "    ax.set_ylabel(\"Delta TS\")\n",
    "    ax.set_title(f\"{par.name}: {par.value:.1e} +- {par.error:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the flux points for the H2 hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flux_points = fpe.run(datasets=[stacked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = spectral_model_ecpl.plot(energy_bounds=[0.1,20]*u.TeV, sed_type=\"e2dnde\", label=\"intrinsic\", color=\"blue\")\n",
    "spectral_model_2.plot(ax=ax, energy_bounds=[0.1,20]*u.TeV, sed_type=\"e2dnde\", label=\"absorbed\", color=\"red\")\n",
    "spectral_model_2.plot_error(ax=ax, energy_bounds=[0.1,20]*u.TeV, sed_type=\"e2dnde\", facecolor=\"red\")\n",
    "flux_points.plot(ax=ax, sed_type=\"e2dnde\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:\n",
    "\n",
    "## Beginner\n",
    "- Select and analyze observations of PSK 2155-304 during its steady state \n",
    "- Try other models, eg: log-parabola, broken power law, etc. See the model gallery for a list of available models: https://docs.gammapy.org/0.19/modeling/gallery/index.html \n",
    "- What is the impact of changing the OFF regions criteria (their number, shape, finding method)? \n",
    "- Try to repeat the fit using a different minimizer. By default Gammapy uses Minuit, but it also supports the Sherpa and Scipy backends.\n",
    "\n",
    "## Advanced\n",
    "- Create a gammapy.estimators.FluxPointsDataset with the flux points you have computed for the stacked dataset and fit the flux points again with obe of the spectral models. How does the result compare to the best fit model, that was directly fitted to the counts data?\n",
    "- Compute a 2-dimensional likelihood contour to estimate the correlation between the fitted parameters (e.g. the spectral index and cutoff). (Tutorial reference: https://docs.gammapy.org/0.20/tutorials/api/fitting.html)\n",
    "- Repeat exercise on the Crab runs available in GAMMAPY_DATA. Alternatively, if you have access to CTA DC1 simulated data, repeat on your favourite source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
